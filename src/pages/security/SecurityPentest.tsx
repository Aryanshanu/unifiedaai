import { useState } from 'react';
import { MainLayout } from '@/components/layout/MainLayout';
import { Card, CardContent } from '@/components/ui/card';
import { Button } from '@/components/ui/button';
import { Select, SelectContent, SelectItem, SelectTrigger, SelectValue } from '@/components/ui/select';
import { Badge } from '@/components/ui/badge';
import { useModels } from '@/hooks/useModels';
import { useToast } from '@/hooks/use-toast';
import { ScanSearch, Loader2, Shield, Layers } from 'lucide-react';
import { supabase } from '@/integrations/supabase/client';
import { useQueryClient } from '@tanstack/react-query';
import { InputOutputScope } from '@/components/engines/InputOutputScope';
import { HealthIndicator } from '@/components/shared/HealthIndicator';
import { useDataHealth } from '@/components/shared/DataHealthWrapper';
import { ComponentErrorBoundary } from '@/components/error/ErrorBoundary';
import { EngineSkeleton } from '@/components/engines/EngineSkeleton';
import { EngineLoadingStatus, EvalStatus } from '@/components/engines/EngineLoadingStatus';
import { EngineErrorCard } from '@/components/engines/EngineErrorCard';
import { NoEndpointWarning } from '@/components/engines/NoModelConnected';
import { VulnerabilityTable } from '@/components/security/VulnerabilityTable';
import { SecurityScoreGauge } from '@/components/security/SecurityScoreGauge';
import { ComputationBreakdown } from '@/components/engines/ComputationBreakdown';
import { EvidencePackage } from '@/components/engines/EvidencePackage';
import { ComplianceBanner } from '@/components/engines/ComplianceBanner';
import { sanitizeErrorMessage } from '@/lib/ui-helpers';

function SecurityPentestContent() {
  const [selectedModelId, setSelectedModelId] = useState('');
  const [evalStatus, setEvalStatus] = useState<EvalStatus>('idle');
  const [evalError, setEvalError] = useState<string | null>(null);
  const [results, setResults] = useState<any>(null);
  const { data: models, isLoading: modelsLoading, refetch: refetchModels } = useModels();
  const { toast } = useToast();
  const queryClient = useQueryClient();

  const selectedModel = models?.find(m => m.id === selectedModelId);
  const hasEndpoint = selectedModel?.huggingface_endpoint || selectedModel?.endpoint;
  const { status, lastUpdated } = useDataHealth(modelsLoading, false);

  const runPentest = async () => {
    if (!selectedModelId) { toast({ title: 'Please select a model', variant: 'destructive' }); return; }
    setEvalStatus('sending');
    setEvalError(null);
    setResults(null);
    try {
      setEvalStatus('analyzing');
      const { data, error } = await supabase.functions.invoke('security-pentest', { body: { modelId: selectedModelId } });
      if (error) throw error;
      setEvalStatus('complete');
      setResults(data);
      queryClient.invalidateQueries({ queryKey: ['security-test-runs'] });
      queryClient.invalidateQueries({ queryKey: ['security-stats'] });
      toast({ title: 'Pentest Complete', description: `Score: ${data.overallScore}%`, variant: data.overallScore >= 70 ? 'default' : 'destructive' });
    } catch (error: any) {
      setEvalStatus('error');
      setEvalError(sanitizeErrorMessage(error.message) || 'Pentest failed');
    }
  };

  if (modelsLoading) return <MainLayout title="AI Pentesting Engine" subtitle="Loading..."><EngineSkeleton /></MainLayout>;

  return (
    <MainLayout title="AI Pentesting Engine" subtitle="OWASP LLM Top 10: Adversarial Input Testing, Prompt Injection, Data Exfiltration"
      headerActions={<HealthIndicator status={status} lastUpdated={lastUpdated} onRetry={refetchModels} showLabel />}>
      <InputOutputScope scope="BOTH" inputDescription="Sends adversarial prompts to model endpoint" outputDescription="Scores vulnerability based on model response analysis" />
      <div className="flex items-center gap-2 mb-4 flex-wrap">
        <Badge className="bg-primary/10 text-primary border-primary/20"><Shield className="w-3 h-3 mr-1" />OWASP LLM Top 10</Badge>
        <Badge variant="outline" className="text-xs"><Layers className="w-3 h-3 mr-1" />10 Test Vectors</Badge>
      </div>
      <Card className="mb-6"><CardContent className="pt-6">
        <div className="flex items-center gap-4">
          <div className="flex-1">
            <label className="text-sm font-medium text-foreground mb-2 block">Select Model</label>
            <Select value={selectedModelId} onValueChange={setSelectedModelId}>
              <SelectTrigger><SelectValue placeholder="Choose a registered model" /></SelectTrigger>
              <SelectContent>{models?.map(m => <SelectItem key={m.id} value={m.id}>{m.name} ({m.model_type})</SelectItem>)}</SelectContent>
            </Select>
            {selectedModelId && !hasEndpoint && <div className="mt-2"><NoEndpointWarning systemId={selectedModel?.system_id} /></div>}
          </div>
          <div className="pt-6">
            <Button onClick={runPentest} disabled={!selectedModelId || (evalStatus !== 'idle' && evalStatus !== 'complete' && evalStatus !== 'error')} size="lg" className="gap-2">
              {evalStatus === 'sending' || evalStatus === 'analyzing' ? <><Loader2 className="w-4 h-4 animate-spin" />Scanning...</> : <><ScanSearch className="w-4 h-4" />Run Pentest</>}
            </Button>
          </div>
        </div>
      </CardContent></Card>
      {evalStatus !== 'idle' && evalStatus !== 'complete' && evalStatus !== 'error' && <div className="mb-6"><EngineLoadingStatus status={evalStatus} engineName="Pentest" /></div>}
      {evalError && <div className="mb-6"><EngineErrorCard type="connection" message={evalError} onRetry={runPentest} isRetrying={evalStatus === 'sending'} systemId={selectedModel?.system_id} /></div>}
      {results && (
        <div className="space-y-6">
          <ComplianceBanner score={results.overallScore} engineName="AI Pentesting" regulatoryReferences={['OWASP LLM Top 10', 'NIST AI RMF', 'EU AI Act Article 9']} />
          <div className="grid grid-cols-1 md:grid-cols-3 gap-6">
            <div className="flex justify-center"><SecurityScoreGauge score={results.overallScore} label="Security Score" size="lg" /></div>
            <Card><CardContent className="pt-6 text-center"><p className="text-3xl font-bold">{results.findings?.length || 0}</p><p className="text-sm text-muted-foreground">Vulnerabilities Found</p></CardContent></Card>
            <Card><CardContent className="pt-6 text-center"><Badge variant={results.riskLevel === 'low' ? 'default' : 'destructive'} className="text-lg px-4 py-2 capitalize">{results.riskLevel}</Badge><p className="text-sm text-muted-foreground mt-2">Risk Level</p></CardContent></Card>
          </div>
          {results.findings?.length > 0 && <Card><CardContent className="pt-6"><h3 className="font-semibold mb-4">Vulnerability Details</h3><VulnerabilityTable findings={results.findings} /></CardContent></Card>}
          {results.computationSteps && <ComputationBreakdown steps={results.computationSteps} overallScore={results.overallScore} weightedFormula="vulnScore = avg(individual_scores)" engineType="security-pentest" euAIActReference="EU AI Act Article 9 - Risk Management" />}
          <EvidencePackage mode="download" data={{ results, rawLogs: results.rawLogs || [], modelId: selectedModelId, evaluationType: 'security-pentest', overallScore: results.overallScore, isCompliant: results.overallScore >= 70 }} />
        </div>
      )}
      {!selectedModelId && <div className="text-center py-16 bg-card rounded-xl border border-border"><ScanSearch className="w-16 h-16 text-muted-foreground/30 mx-auto mb-4" /><h3 className="text-lg font-semibold text-foreground mb-2">Select a Model</h3><p className="text-muted-foreground">Choose a model to run security pentesting</p></div>}
    </MainLayout>
  );
}

export default function SecurityPentest() { return <ComponentErrorBoundary><SecurityPentestContent /></ComponentErrorBoundary>; }
